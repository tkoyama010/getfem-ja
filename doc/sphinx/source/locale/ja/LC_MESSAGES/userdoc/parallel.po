# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2004-2017 GetFEM++ project
# This file is distributed under the same license as the GetFEM++ package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: GetFEM++ 5.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-06-22 15:02+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.1\n"

# 78968ee6e29c431b94bbda4c753fcd92
#: ../source/userdoc/parallel.rst:10
msgid "MPI Parallelization of |gf|"
msgstr ""

# e0aa8b97a25f4e80a4303a93d410f23a
#: ../source/userdoc/parallel.rst:12
msgid ""
"Of course, each different problem should require a different "
"parallelization adapted to its specificities in order to obtain a good "
"load balancing. You may build your own parallelization using the mesh "
"regions to parallelize assembly procedures."
msgstr ""

# e8b6e00ed2174c4fb815a2f08ef5154c
#: ../source/userdoc/parallel.rst:17
msgid ""
"Nevertheless, the brick system offers a generic parallelization based on "
"MPI (communication between processes), `METIS "
"<http://glaros.dtc.umn.edu/gkhome/metis/metis/overview>`_ (partition of "
"the mesh) and `MUMPS <http://graal.ens-lyon.fr/MUMPS>`_ (parallel sparse "
"direct solver). It is available with the compiler option ``-D "
"GETFEM_PARA_LEVEL=2`` and the library itself has to be compiled with the "
"option ``--enable-paralevel=2`` of the configure script. Initial MPI "
"parallelization of |gf| has been designed with the help of Nicolas Renon "
"from CALMIP, Toulouse."
msgstr ""

# 956802b68a9847b4b40994a0e8ffaace
#: ../source/userdoc/parallel.rst:28
msgid ""
"When the configure script is run with the option ``--enable-"
"paralevel=2``, it searches for MPI, METIS and parallel MUMPS libraries. "
"If the python interface is built, it searches also for MPI4PY library. In"
" that case, the python interface can be used to drive the parallel "
"version of getfem (the other interfaces has not been parallelized for the"
" moment). See demo_parallel_laplacian.py in the interface/test/python "
"directory."
msgstr ""

# e794b3487bc3472fba2d325c3cd0571e
#: ../source/userdoc/parallel.rst:36
msgid ""
"With the option ``-D GETFEM_PARA_LEVEL=2``, each mesh used is implicitely"
" partitionned (using METIS) into a number of regions corresponding to the"
" number of processors and the assembly procedures are parallelized. This "
"means that the tangent matrix and the constraint matrix assembled in the "
"model_state variable are distributed. The choice made (for the moment) is"
" not to distribute the vectors. So that the right hand side vectors in "
"the model_state variable are communicated to each processor (the sum of "
"each contribution is made at the end of the assembly and each processor "
"has the complete vector). Note that you have to think to the fact that "
"the matrices stored by the bricks are all distributed."
msgstr ""

# 572147d0ae9144d984553078be78cc7c
#: ../source/userdoc/parallel.rst:48
msgid ""
"A model of C++ parallelized program is :file:`tests/elastostatic.cc`. To "
"run it in parallel you have to lauch for instance::"
msgstr ""

# 616340bd62e641d1aad4d1cbe9757518
#: ../source/userdoc/parallel.rst:53
msgid "For a python interfaced program, the call reads::"
msgstr ""

# cabf27dbc2aa43fcb6226789d76cd823
#: ../source/userdoc/parallel.rst:57
msgid ""
"If you do not perform a `make install`, do not forget to first set the "
"shell variable PYTHONPATH to the python-getfem library with for "
"instance::"
msgstr ""

# eb1c3fc3228946b9bdf090090fca7f2e
#: ../source/userdoc/parallel.rst:63
msgid "State of progress of |gf| MPI parallelization"
msgstr ""

# 4ed2bbe0436549e19077e70581bcd7a5
#: ../source/userdoc/parallel.rst:65
msgid ""
"Parallelization of getfem is still considered a \"work in progress\". A "
"certain number of procedure are still remaining sequential. Of course, a "
"good test to see if the parallelization of your program is correct is to "
"verify that the result of the computation is indeed independent of the "
"number of process."
msgstr ""

# 69e8d4e43fcc45b7bb9a293cf65aec88
#: ../source/userdoc/parallel.rst:67
msgid "Assembly procedures"
msgstr ""

# 518f1f2b3cb64aebb7f3609b87872525
#: ../source/userdoc/parallel.rst:69
msgid ""
"Most of assembly procedures (in :file:`getfem/getfem_assembling.h`) have "
"a parameter corresponding to the region in which the assembly is to be "
"computed. They are not parallelized themselves but aimed to be called "
"with a different region in each process to distribute the job. Note that "
"the file :file:`getfem/getfem_config.h` contains a procedures called "
"MPI_SUM_SPARSE_MATRIX allowing to gather the contributions of a "
"distributed sparse matrix."
msgstr ""

# 4d671431b64647cab8eafcf28819ee8c
#: ../source/userdoc/parallel.rst:71
msgid ""
"The following assembly procedures are implicitely parallelized using the "
"option ``-D GETFEM_PARA_LEVEL=2``:"
msgstr ""

# 5cd1c931dea44b0781f50d56417353db
#: ../source/userdoc/parallel.rst:73
msgid ""
"computation of norms (``asm_L2_norm``, ``asm_H1_norm``, ``asm_H2_norm`` "
"..., in :file:`getfem/getfem_assembling.h`),"
msgstr ""

# d5ae0c143bbe4ec1a3709923a22fd58a
#: ../source/userdoc/parallel.rst:75
msgid "``asm_mean_value`` (in :file:`getfem/getfem_assembling.h`),"
msgstr ""

# ec234575def84dbfbb7ac4bcd073b596
#: ../source/userdoc/parallel.rst:77
msgid "``error_estimate`` (in :file:`getfem/getfem_error_estimate.h`)."
msgstr ""

# b352c562f5094d14866c2389cbeaa255
#: ../source/userdoc/parallel.rst:79
msgid ""
"This means in particular that these functions have to be called on each "
"processor."
msgstr ""

# 0cab3c2e37fb49c3bc52b7e4d7bfc4ab
#: ../source/userdoc/parallel.rst:82
msgid "Mesh_fem object"
msgstr ""

# 01c9d4682ba747eeb7e1416264d36251
#: ../source/userdoc/parallel.rst:84
msgid ""
"The dof numbering of the getfem::mesh_fem object remains sequential and "
"is executed on each process.  The parallelization is to be done. This "
"could affect the efficiency of the parallelization for very large and/or "
"evoluting meshes."
msgstr ""

# 3f70456c541142d7a8787024740e9b41
#: ../source/userdoc/parallel.rst:87
msgid "Model object and bricks"
msgstr ""

# 20779c4c294c45dcb8f9507844ad5b48
#: ../source/userdoc/parallel.rst:89
msgid ""
"The model system is globally parallelized, which mainly means that the "
"assembly procedures of standard bricks use a METIS partition of the "
"meshes to distribute the assembly. The tangent/stiffness matrices remain "
"distibuted and the standard solve call the parallel version of MUMPS "
"(which accept distributed matrices)."
msgstr ""

# 538a2c6f5d5b41958d52ed26bea31371
#: ../source/userdoc/parallel.rst:95
msgid ""
"For the moment, the procedure ``actualize_sizes()`` of the model object "
"remains sequential and is executed on each process. The parallelization "
"is to be done."
msgstr ""

# 38080f669d70482fa0ac6a395c56ca18
#: ../source/userdoc/parallel.rst:99
msgid "Some specificities:"
msgstr ""

# 10636628aca54316ba9aee1520ca8b4d
#: ../source/userdoc/parallel.rst:101
msgid ""
"The explicit matrix brick: the given matrix is considered to be "
"distributed. If it is not, only add it on the master process (otherwize, "
"the contribution will be multiplied by the numberof processes)."
msgstr ""

# cfe88f1d6db743119966dedffed1803e
#: ../source/userdoc/parallel.rst:105
msgid ""
"The explicit rhs brick: the given vector is not considered to be "
"distributed. Only the given vector on the master process is taken into "
"account."
msgstr ""

# c1e079f8c8c3430cb0c94d42b932796c
#: ../source/userdoc/parallel.rst:109
msgid ""
"Constraint brick: The given mastrix and rhs are not considered to be "
"distributed. Only the given matrix and vector on the master process are "
"taken into account."
msgstr ""

# d37b51baf5cf40ffb81e38595d7ef7cc
#: ../source/userdoc/parallel.rst:113
msgid ""
"Concerning contact bricks, only integral contact bricks are fully "
"parallelized for the moment. Nodal contact bricks work in parallel but "
"all the computation is done on the master process."
msgstr ""

